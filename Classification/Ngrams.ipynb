{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RswUUvuKwJOo",
        "outputId": "956d6779-8ff5-4cf9-8f06-505b557ffa64"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-03 13:33:25.715235: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-09-03 13:33:25.761597: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-09-03 13:33:26.836873: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import regex as re\n",
        "import spacy\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "files_path = Path(\"/home/hombre/Code/Climate Project/Classification/files\")\n",
        "#nlp = spacy.load('pl_core_news_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sg8tYSeDwJOs"
      },
      "outputs": [],
      "source": [
        "stopwords = pd.read_csv(files_path / 'polish_stopwords.txt', header=None)\n",
        "STOPWORDS = set([word.rstrip() for word in stopwords[0]])\n",
        "\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;\\.]')\n",
        "REPLACE_EXCESS_SPACES_RE = re.compile('\\s\\s+')\n",
        "BAD_SYMBOLS_RE = re.compile('[^AaĄąBbCcĆćDdEeĘęFfGgHhIiJjKkLlŁłMmNnŃńOoÓóPpRrSsŚśTtUuWwYyZzŹźŻż1234567890 ]')\n",
        "\n",
        "def lemmatizer(text):\n",
        "  return \n",
        "  doc = nlp(text)\n",
        "  return \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "\n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = REPLACE_EXCESS_SPACES_RE.sub(' ', text)\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # No Stemming\n",
        "    #text = ' '.join(stemmer.stem(word) for word in text.split() if word not in STOPWORDS and stemmer.stem(word) != None) # Stemming\n",
        "    if text == None:\n",
        "      text == \"None None None\"\n",
        "    return text\n",
        "\n",
        "def preprocess(df):\n",
        "    df['title'] = df['title'].fillna(\"None\")\n",
        "    df['clean_text'] = df['text'].apply(clean_text)\n",
        "    df['clean_title'] = df['title'].apply(clean_text)\n",
        "    df['clean_title'] = df['clean_title'].apply(lemmatizer)\n",
        "    df['clean_text'] = df['clean_text'].apply(lemmatizer)\n",
        "    return df\n",
        "\n",
        "\n",
        "def generate_ngrams(text, n_gram=1):\n",
        "    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n",
        "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
        "    return [' '.join(ngram) for ngram in ngrams]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "weak_grams= pd.read_csv(\"files/ngram/mis_ngrams.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_results = pd.read_csv(files_path / \"rzepa\"/ \"rare.csv\")\n",
        "\n",
        "#df_results = df_results[df_results['label'] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ngram_counter(ngram, df):\n",
        "  eco_ngrams = defaultdict(int)\n",
        "\n",
        "  for article in df['clean_text']:\n",
        "      for word in generate_ngrams(article, ngram):\n",
        "          eco_ngrams[word] += 1\n",
        "        \n",
        "          \n",
        "  df_eco_ngrams = pd.DataFrame(sorted(eco_ngrams.items(), key=lambda x: x[1])[::-1])\n",
        "\n",
        "  return df_eco_ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_eco_unigrams = ngram_counter(1, df_results)\n",
        "df_eco_bigrams = ngram_counter(2, df_results)\n",
        "df_eco_trigrams = ngram_counter(3, df_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NDA4aPO0wJOx"
      },
      "outputs": [],
      "source": [
        "df_1 =pd.read_csv(files_path/ \"ngram/unigrams.csv\")\n",
        "df_2 =pd.read_csv(files_path/ \"ngram/bigrams.csv\")\n",
        "df_3 =pd.read_csv(files_path/ \"ngram/trigrams.csv\")\n",
        "\n",
        "df_eco = pd.concat([df_1, df_2, df_3])\n",
        "\n",
        "eco_dict = df_eco.set_index(\"phrase\").to_dict()['count']\n",
        "eco_vocab = dict(zip(list(eco_dict.keys()), range(len(eco_dict.keys()))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_eco_unigrams[~df_eco_unigrams[0].isin(eco_vocab.keys())].iloc[:400].to_csv(files_path/ \"ngram/uni_rest.csv\", index = False)\n",
        "df_eco_bigrams[~df_eco_bigrams[0].isin(eco_vocab.keys())].iloc[:400].to_csv(files_path/ \"ngram/bi_rest.csv\", index = False)\n",
        "df_eco_trigrams[~df_eco_trigrams[0].isin(eco_vocab.keys())].iloc[:400].to_csv(files_path/ \"ngram/tri_rest.csv\", index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "usr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
