{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from classify import data_pipeline, eco_selector\n",
        "from resources.get_data import get_current_data\n",
        "from resources.setup import get_setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Big corpuses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rzepa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corp = 'rzepa'\n",
        "df = data_pipeline(corp)\n",
        "df_eco = eco_selector(df)\n",
        "df_eco.to_csv(f\"eco_{corp}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wyborcza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corp = 'wyborcza'\n",
        "df = data_pipeline(corp)\n",
        "df_eco = eco_selector(df, True)\n",
        "df_eco.to_csv(f\"eco_{corp}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gazeta Polska Codziennie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corp = 'gpc'\n",
        "df = data_pipeline(corp)\n",
        "df_eco = eco_selector(df, ease=True)\n",
        "df_eco.to_csv(f\"eco_{corp}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Polityka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corp = 'polityka'\n",
        "df = data_pipeline(corp)\n",
        "df_eco = eco_selector(df, ease=True)\n",
        "df_eco.to_csv(f\"eco_{corp}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Small Corpuses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dorzeczy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: 'rzepa', 2: 'gpc', 3: 'newsweek', 4: 'wprost', 5: 'dorzeczy', 6: 'polityka', 7: 'wyborcza', 8: 'wpolityce'}\n",
            "dorzeczy\n",
            "Configuration finished\n",
            "Dataset of 17711 samples\n",
            "0    8954\n",
            "1    8757\n",
            "Name: label, dtype: int64\n",
            "Preprocessing ended\n",
            "Accuracy of model 0.9971783295711061\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:05<00:00,  2.71s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "978 368\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "368"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corp = 'dorzeczy'\n",
        "df = data_pipeline(corp)\n",
        "df_eco = eco_selector(df, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_eco.to_csv(f\"eco_{corp}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wprost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: 'rzepa', 2: 'gpc', 3: 'newsweek', 4: 'wprost', 5: 'dorzeczy', 6: 'polityka', 7: 'wyborcza', 8: 'wpolityce'}\n",
            "wprost\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/hombre/Code/Climate Project/Classification/resources/get_data.py:7: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
            "  df_eco = pd.read_csv(drive_path / corpus / \"eco_result.csv\", index_col=['id', 'source'], parse_dates=['date'])\n",
            "/home/hombre/Code/Climate Project/Classification/resources/get_data.py:8: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
            "  df_non_eco = pd.read_csv(drive_path / corpus / \"non_eco_result.csv\", index_col = ['id', 'source'], parse_dates=['date'])\n"
          ]
        }
      ],
      "source": [
        "corp = 'wprost'\n",
        "configuration = get_setup(corp)\n",
        "df_eco, df_non_eco, df_rest = get_current_data(configuration)\n",
        "pd.concat([df_eco, df_rest.drop(index=2260)]).to_csv(f\"eco_{corp}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Newsweek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: 'rzepa', 2: 'gpc', 3: 'newsweek', 4: 'wprost', 5: 'dorzeczy', 6: 'polityka', 7: 'wyborcza', 8: 'wpolityce'}\n",
            "newsweek\n"
          ]
        }
      ],
      "source": [
        "corp = 'newsweek'\n",
        "configuration = get_setup(corp)\n",
        "df_eco, df_non_eco, df_rest = get_current_data(configuration)\n",
        "df_all = pd.concat([df_rest['words'].drop(index=2051), df_eco])\n",
        "df_all.to_csv(f\"eco_{corp}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## wPolityce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: 'rzepa', 2: 'gpc', 3: 'newsweek', 4: 'wprost', 5: 'dorzeczy', 6: 'polityka', 7: 'wyborcza', 8: 'wpolityce'}\n",
            "wpolityce\n"
          ]
        }
      ],
      "source": [
        "corp = 'wpolityce'\n",
        "configuration = get_setup(corp)\n",
        "df_eco, df_non_eco, df_rest = get_current_data(configuration)\n",
        "df_eco.to_csv(f\"eco_{corp}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14609\n",
            "12215235\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def read_and_drop_vectorized(directory_path):\n",
        "    csv_files = [file for file in os.listdir(directory_path) if file.endswith('.csv')]\n",
        "\n",
        "    dataframes = []\n",
        "    corpus_len = 0\n",
        "    tokens = 0\n",
        "    for csv_file in csv_files:\n",
        "        file_path = os.path.join(directory_path, csv_file)\n",
        "        \n",
        "        df = pd.read_csv(file_path)\n",
        "        if 'vectorized' in df.columns:\n",
        "            df = df.drop(columns=['vectorized'])\n",
        "\n",
        "        if df.index.duplicated().any():\n",
        "            print(f\"Warning: Duplicated index found in file {csv_file}\")\n",
        "            df = df.reset_index()\n",
        "            df = df.drop(columns=['id'])\n",
        "            df = df.rename(columns={\"index\": 'id'})\n",
        "        \n",
        "        corpus_len += len(df)\n",
        "        tokens += df['text'].apply(lambda x: len(x.split(\" \"))).sum()\n",
        "        dataframes.append(df)\n",
        "    print(corpus_len)\n",
        "    print(tokens)\n",
        "    return dataframes\n",
        "\n",
        "# Example usage:\n",
        "directory_path = '.'\n",
        "corpus_df = read_and_drop_vectorized(directory_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
