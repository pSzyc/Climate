{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7xcqUYs9L9s",
        "outputId": "56e9f079-cddd-4805-a691-24093b801300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-15 17:49:35.296589: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 17:49:35.301959: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 17:49:35.302033: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 17:49:39.037192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting pl-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pl_core_news_sm-3.6.0/pl_core_news_sm-3.6.0-py3-none-any.whl (20.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from pl-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->pl-core-news-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: pl-core-news-sm\n",
            "Successfully installed pl-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pl_core_news_sm')\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "from multiprocessing import Pool\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "!python -m spacy download pl_core_news_sm\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "nlp = spacy.load('pl_core_news_sm')\n",
        "corpus_path = Path(\"/content/drive/Shareddrives/SKN AI FUW /Projekt z Dagmarą Mateją/Korpusy\")\n",
        "materials_path = Path(\"/content/drive/Shareddrives/SKN AI FUW /Projekt z Dagmarą Mateją/NLP materials\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i4_r4mHt9L9w"
      },
      "outputs": [],
      "source": [
        "stopwords = pd.read_csv( materials_path / 'polish_stopwords.txt', header=None)\n",
        "STOPWORDS = set([word.rstrip() for word in stopwords[0]])\n",
        "links_gw = [corpus_path / \"Gazeta Wyborcza (do 2018 r.)\" / \"data\" / f\"wyborcza_{i}.csv\" for i in range(22)]\n",
        "links_wprost = [corpus_path / \"Wprost 2015-2022\" / \"data\" / f\"Wprost.csv\"]\n",
        "links_newsweek = [corpus_path / \"Newsweek 2015-2022\" / 'data' / f\"newsweek.csv\"]\n",
        "links_dorzeczy = [corpus_path / \"DoRzeczy\" / \"data\" / \"dorzeczy.csv\"]\n",
        "links_polityka = [corpus_path / \"Polityka\" / \"data\" / f\"polityka_{i}.csv\" for i in range(5)]\n",
        "links_gpc = [corpus_path / \"Gazeta Polska Codziennie\" / \"data\" / f\"gpc_{i}.csv\" for i in range(13)]\n",
        "links_rzepa = [corpus_path / \"Rzeczpospolita 2015-2022\" / \"data\"/ f\"rzepa_{i}.csv\" for i in range(22)]\n",
        "links_dataset = [materials_path / \"dataset.csv\"]\n",
        "links_wpolityce = [corpus_path / \"wPolityce\" / 'data' / \"wPolityce.csv\" ]\n",
        "\n",
        "corpus_links = {\n",
        "    \"newsweek\" : links_newsweek,\n",
        "    \"rzepa\" : links_rzepa,\n",
        "    \"gpc\" : links_gpc,\n",
        "    \"wprost\" : links_wprost,\n",
        "    \"polityka\" : links_polityka,\n",
        "    \"dorzeczy\" : links_dorzeczy,\n",
        "    \"dataset\" : links_dataset,\n",
        "    \"wyborcza\" : links_gw,\n",
        "    \"wpolityce\" : links_wpolityce\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KMz4-XW89L9x"
      },
      "outputs": [],
      "source": [
        "def generate_ngrams(text, n_gram=1):\n",
        "    try:\n",
        "        token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n",
        "    except:\n",
        "        token = [\"Nan\", \"Nan\", \"Nan\"]\n",
        "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
        "    return [' '.join(ngram) for ngram in ngrams]\n",
        "\n",
        "def get_ngrams():\n",
        "    df_ngram = pd.read_csv(materials_path / \"ngram/ngrams.csv\")\n",
        "    df_weak = pd.read_csv(materials_path / \"ngram\" / \"weak_grams.csv\")\n",
        "    df_ngram = df_ngram.reset_index().set_index(\"phrase\").rename(columns = {\"index\":\"id\"})\n",
        "    return df_ngram, df_weak\n",
        "\n",
        "def get_vocab(df_ngram):\n",
        "    eco_vocab = dict(zip(df_ngram.index, df_ngram.id))\n",
        "    topic_vocab = dict(zip(df_ngram.index, df_ngram.origin))\n",
        "    return eco_vocab, topic_vocab\n",
        "\n",
        "def get_topic_counter(df_ngram):\n",
        "  return dict(zip(df_ngram.origin.unique(), np.zeros_like(df_ngram.origin.unique())))\n",
        "\n",
        "def count_vectorizer(article):\n",
        "    vect = np.zeros(len(eco_vocab)).tolist()\n",
        "    topic_counter = get_topic_counter(df_ngram)\n",
        "    weak_count = 0\n",
        "    for ngram in range(1,4):\n",
        "      for word in generate_ngrams(article['clean_text'], ngram):\n",
        "          if word in eco_vocab:\n",
        "            vect[eco_vocab[word]] += ngram\n",
        "            origin = topic_vocab[word]\n",
        "            topic_counter[origin] += ngram\n",
        "          if word in df_weak.phrase.values:\n",
        "            weak_count += ngram\n",
        "\n",
        "      for word in generate_ngrams(article['clean_title'], ngram):\n",
        "          if word in eco_vocab:\n",
        "            vect[eco_vocab[word]] += 2*ngram\n",
        "\n",
        "    ngram_sum = np.sum(vect)\n",
        "    ngram_sum_squared = ngram_sum ** 2\n",
        "    ngram_sum_squared_to_total = ngram_sum ** 2 / len(article['clean_text'].split(' '))\n",
        "    topics = np.array(list(topic_counter.values()))\n",
        "\n",
        "    if ngram_sum != 0:\n",
        "      topics = topics / ngram_sum\n",
        "      weak_count /= ngram_sum\n",
        "\n",
        "    stats = np.array([ngram_sum, ngram_sum_squared, ngram_sum_squared_to_total, weak_count])\n",
        "\n",
        "    vect = np.concatenate((vect, stats, topics))\n",
        "    return vect\n",
        "\n",
        "\n",
        "def get_word_list(vectorized):\n",
        "    vectorized = vectorized[:len(eco_vocab)]\n",
        "    indicies = np.nonzero(vectorized)[0]\n",
        "    words = []\n",
        "\n",
        "    for i in indicies:\n",
        "        if i < len(eco_vocab):\n",
        "            words.extend([phrase for phrase in eco_vocab.keys() if eco_vocab[phrase] == i])\n",
        "\n",
        "    return words\n",
        "\n",
        "df_ngram, df_weak = get_ngrams()\n",
        "stat_list = [\n",
        "   'ngram_sum', 'ngram_sum_squared', 'ngram_sum_squared_to_total', 'weak_count'\n",
        "   ] + list(get_topic_counter(df_ngram).keys())\n",
        "\n",
        "assert len(df_ngram[df_ngram.index.duplicated()]) == 0\n",
        "eco_vocab, topic_vocab = get_vocab(df_ngram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_RH-HEaA9L9y"
      },
      "outputs": [],
      "source": [
        "def get_model(df):\n",
        "    if \"vectorized\" not in df.columns.values:\n",
        "      df['vectorized'] = df.apply(count_vectorizer, axis=1)\n",
        "    df_train, df_test = train_test_split(df)\n",
        "    train_x = df_train['vectorized'].to_list()\n",
        "    test_x = df_test['vectorized'].to_list()\n",
        "    train_y = df_train['label']\n",
        "    test_y = df_test['label']\n",
        "\n",
        "    train_x = np.array(train_x)\n",
        "    test_x = np.array(test_x)\n",
        "    params = {'n_estimators': 386, 'max_depth': 13, 'min_samples_split': 9}\n",
        "    rf = RandomForestClassifier(**params)\n",
        "    rf.fit(train_x, train_y)\n",
        "    y_proba = rf.predict_proba(test_x)\n",
        "    y_pred = y_proba[:,1] > 0.50\n",
        "    print(precision_score(test_y, y_pred))\n",
        "    return rf\n",
        "\n",
        "def naive_filter(link, rf):\n",
        "    df = pd.read_csv(link)  # Changed index=\"id\" to index_col=\"id\"\n",
        "    df['clean_title'] = df['clean_title'].fillna(\"None None None\")\n",
        "    df['clean_text'] = df['clean_text'].fillna(\"None None None\")\n",
        "    df['vectorized'] = df.apply(count_vectorizer, axis=1)\n",
        "    x_data = np.array(df['vectorized'].tolist())\n",
        "    df['proba'] = rf.predict_proba(x_data)[:,1]\n",
        "    df['words'] = df['vectorized'].apply(get_word_list)\n",
        "    df['num_words'] = df['words'].apply(lambda x: len(x))\n",
        "    return df\n",
        "\n",
        "def get_trainset(corpus):\n",
        "  if corpus == \"rzepa\":\n",
        "    df = pd.read_csv(links_dataset[0], index_col=['id', 'translated'])\n",
        "    df.rename(columns = {\"class\" : \"label\"}, inplace = True)\n",
        "  else:\n",
        "    df_eco = pd.read_csv(materials_path / \"rzepa\" / \"eco_rzepa.csv\", index_col='id')\n",
        "    df_non_eco = pd.read_csv(materials_path / \"rzepa\" / \"non_eco_result.csv\", index_col='id')\n",
        "    df_eco['label'] = 1\n",
        "    df_non_eco['label'] = 0\n",
        "    df = pd.concat([df_eco, df_non_eco])\n",
        "    df['vectorized'] = df['vectorized'].apply(lambda x: x[1:-1].split(\" \"))\n",
        "    df['vectorized'] = df['vectorized'].apply(lambda data_list: [float(x) for x in data_list if x != ''])\n",
        "  df = df.fillna(\"None\")\n",
        "  return df\n",
        "\n",
        "def get_resulting_df(links, rf, corpus):\n",
        "    result = []\n",
        "    for link in tqdm(links):\n",
        "       result.append(naive_filter(link, rf))\n",
        "    df_result = pd.concat(result).set_index('id')\n",
        "    if len(df_result) < 5 * 10 ** 4  and corpus != \"dorzeczy\":\n",
        "      return df_result\n",
        "    else:\n",
        "      print(f\"{corpus}: {len(df_result)}\")\n",
        "      return df_result[df_result['proba'] > 0.1]\n",
        "\n",
        "def get_statistics(df_result):\n",
        "    for i, key in enumerate(stat_list[::-1]):\n",
        "      df_result[key] = df_result['vectorized'].apply(lambda x: x[-(i + 1)])\n",
        "    return df_result\n",
        "\n",
        "def save(corpus, df_result, df_eco, df_non_eco):\n",
        "    (materials_path / corpus).mkdir(parents=True, exist_ok=True)\n",
        "    df_eco.to_csv(materials_path/ corpus /\"eco_result.csv\")\n",
        "    df_non_eco.to_csv(materials_path/ corpus /\"non_eco_result.csv\")\n",
        "    df_result.to_csv(materials_path/ corpus /\"results.csv\")\n",
        "\n",
        "def determine_fate(df_result, corpus):\n",
        "  if corpus == 'rzepa':\n",
        "    df_eco = df_result[(df_result['proba'] > .9) & (df_result['ngram_sum_squared_to_total'] > 0.75) & (df_result['num_words']  > 3)]\n",
        "    df_non_eco = df_result[df_result['proba'] <= .25]\n",
        "  else:\n",
        "    df_eco = df_result[(df_result['proba'] > .5) & (df_result['ngram_sum_squared_to_total'] > 0.5) & (df_result['num_words']  > 2)]\n",
        "    df_non_eco = df_result[df_result['proba'] <= .2]\n",
        "  df_result = df_result[~ (df_result.index.isin(df_non_eco.index.values) | df_result.index.isin(df_eco.index.values)) ]\n",
        "  print((len(df_result), len(df_eco), len(df_non_eco)))\n",
        "  return df_result,df_eco,df_non_eco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFyAj2of9L9z",
        "outputId": "ccbf4d23-c642-4229-b5e4-17a102655de8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done: newsweek\n",
            "Done: rzepa\n",
            "Done: gpc\n",
            "Done: wprost\n",
            "Done: polityka\n",
            "Processing: dorzeczy\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [08:44<00:00, 524.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dorzeczy: 11795\n",
            "(688, 290, 243)\n",
            "Done: wyborcza\n",
            "Done: wpolityce\n"
          ]
        }
      ],
      "source": [
        "def preprocess_corpus(corpus):\n",
        "    if (materials_path / corpus / \"eco_result.csv\").is_file() | (materials_path / corpus / \"non_eco_result.csv\").is_file() | (materials_path / corpus / \"results.csv\").is_file():\n",
        "      print(\"Done: \" + corpus)\n",
        "      return\n",
        "    else:\n",
        "      print(\"Processing: \" + corpus)\n",
        "    links = corpus_links[corpus]\n",
        "    df = get_trainset(corpus)\n",
        "    rf = get_model(df)\n",
        "    df_result = get_resulting_df(links, rf, corpus)\n",
        "    get_statistics(df_result)\n",
        "    df_result, df_eco, df_non_eco = determine_fate(df_result, corpus)\n",
        "    save(corpus, df_result, df_eco, df_non_eco)\n",
        "\n",
        "\n",
        "corpus_links = {\n",
        "    \"newsweek\" : links_newsweek,\n",
        "    \"rzepa\" : links_rzepa,\n",
        "    \"gpc\" : links_gpc,\n",
        "    \"wprost\" : links_wprost,\n",
        "    \"polityka\" : links_polityka,\n",
        "    \"dorzeczy\" : links_dorzeczy,\n",
        "    \"dataset\" : links_dataset,\n",
        "    \"wyborcza\" : links_gw,\n",
        "    \"wpolityce\" : links_wpolityce\n",
        "}\n",
        "for corpus in corpus_links.keys():\n",
        "  if corpus != 'dataset':\n",
        "    preprocess_corpus(corpus)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}