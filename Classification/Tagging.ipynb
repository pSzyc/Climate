{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7xcqUYs9L9s",
        "outputId": "56e9f079-cddd-4805-a691-24093b801300"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "from multiprocessing import Pool\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "!python -m spacy download pl_core_news_sm\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "nlp = spacy.load('pl_core_news_sm')\n",
        "corpus_path = Path(\"/content/drive/Shareddrives/SKN AI FUW /Projekt z Dagmara台 Mateja台/Korpusy\")\n",
        "materials_path = Path(\"/content/drive/Shareddrives/SKN AI FUW /Projekt z Dagmara台 Mateja台/NLP materials\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4_r4mHt9L9w"
      },
      "outputs": [],
      "source": [
        "stopwords = pd.read_csv( materials_path / 'polish_stopwords.txt', header=None)\n",
        "STOPWORDS = set([word.rstrip() for word in stopwords[0]])\n",
        "links_gw = [corpus_path / \"Gazeta Wyborcza (do 2018 r.)\" / \"data\" / f\"wyborcza_{i}.csv\" for i in range(22)]\n",
        "links_wprost = [corpus_path / \"Wprost 2015-2022\" / \"data\" / f\"Wprost.csv\"]\n",
        "links_newsweek = [corpus_path / \"Newsweek 2015-2022\" / 'data' / f\"newsweek.csv\"]\n",
        "links_dorzeczy = [corpus_path / \"DoRzeczy\" / \"data\" / \"dorzeczy.csv\"]\n",
        "links_polityka = [corpus_path / \"Polityka\" / \"data\" / f\"polityka_{i}.csv\" for i in range(5)]\n",
        "links_gpc = [corpus_path / \"Gazeta Polska Codziennie\" / \"data\" / f\"gpc_{i}.csv\" for i in range(13)]\n",
        "links_rzepa = [corpus_path / \"Rzeczpospolita 2015-2022\" / \"data\"/ f\"rzepa_{i}.csv\" for i in range(22)]\n",
        "links_dataset = [materials_path / \"dataset.csv\"]\n",
        "links_wpolityce = [corpus_path / \"wPolityce\" / 'data' / \"wPolityce.csv\" ]\n",
        "\n",
        "corpus_links = {\n",
        "    \"newsweek\" : links_newsweek,\n",
        "    \"rzepa\" : links_rzepa,\n",
        "    \"gpc\" : links_gpc,\n",
        "    \"wprost\" : links_wprost,\n",
        "    \"polityka\" : links_polityka,\n",
        "    \"dorzeczy\" : links_dorzeczy,\n",
        "    \"dataset\" : links_dataset,\n",
        "    \"wyborcza\" : links_gw,\n",
        "    \"wpolityce\" : links_wpolityce\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMz4-XW89L9x"
      },
      "outputs": [],
      "source": [
        "def generate_ngrams(text, n_gram=1):\n",
        "    try:\n",
        "        token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n",
        "    except:\n",
        "        token = [\"Nan\", \"Nan\", \"Nan\"]\n",
        "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
        "    return [' '.join(ngram) for ngram in ngrams]\n",
        "\n",
        "def get_ngrams():\n",
        "    df_ngram = pd.read_csv(materials_path / \"ngram/ngrams.csv\")\n",
        "    df_weak = pd.read_csv(materials_path / \"ngram\" / \"weak_grams.csv\")\n",
        "    df_ngram = df_ngram.reset_index().set_index(\"phrase\").rename(columns = {\"index\":\"id\"})\n",
        "    return df_ngram, df_weak\n",
        "\n",
        "def get_vocab(df_ngram):\n",
        "    eco_vocab = dict(zip(df_ngram.index, df_ngram.id))\n",
        "    topic_vocab = dict(zip(df_ngram.index, df_ngram.origin))\n",
        "    return eco_vocab, topic_vocab\n",
        "\n",
        "def get_topic_counter(df_ngram):\n",
        "  return dict(zip(df_ngram.origin.unique(), np.zeros_like(df_ngram.origin.unique())))\n",
        "\n",
        "def count_vectorizer(article):\n",
        "    vect = np.zeros(len(eco_vocab)).tolist()\n",
        "    topic_counter = get_topic_counter(df_ngram)\n",
        "    weak_count = 0\n",
        "    for ngram in range(1,4):\n",
        "      for word in generate_ngrams(article['clean_text'], ngram):\n",
        "          if word in eco_vocab:\n",
        "            vect[eco_vocab[word]] += ngram\n",
        "            origin = topic_vocab[word]\n",
        "            topic_counter[origin] += ngram\n",
        "          if word in df_weak.phrase.values:\n",
        "            weak_count += ngram\n",
        "\n",
        "      for word in generate_ngrams(article['clean_title'], ngram):\n",
        "          if word in eco_vocab:\n",
        "            vect[eco_vocab[word]] += 2*ngram\n",
        "\n",
        "    ngram_sum = np.sum(vect)\n",
        "    ngram_sum_squared = ngram_sum ** 2\n",
        "    ngram_sum_squared_to_total = ngram_sum ** 2 / len(article['clean_text'].split(' '))\n",
        "    topics = np.array(list(topic_counter.values()))\n",
        "\n",
        "    if ngram_sum != 0:\n",
        "      topics = topics / ngram_sum\n",
        "      weak_count /= ngram_sum\n",
        "\n",
        "    stats = np.array([ngram_sum, ngram_sum_squared, ngram_sum_squared_to_total, weak_count])\n",
        "\n",
        "    vect = np.concatenate((vect, stats, topics))\n",
        "    return vect\n",
        "\n",
        "\n",
        "def get_word_list(vectorized):\n",
        "    vectorized = vectorized[:len(eco_vocab)]\n",
        "    indicies = np.nonzero(vectorized)[0]\n",
        "    words = []\n",
        "\n",
        "    for i in indicies:\n",
        "        if i < len(eco_vocab):\n",
        "            words.extend([phrase for phrase in eco_vocab.keys() if eco_vocab[phrase] == i])\n",
        "\n",
        "    return words\n",
        "\n",
        "df_ngram, df_weak = get_ngrams()\n",
        "stat_list = [\n",
        "   'ngram_sum', 'ngram_sum_squared', 'ngram_sum_squared_to_total', 'weak_count'\n",
        "   ] + list(get_topic_counter(df_ngram).keys())\n",
        "\n",
        "assert len(df_ngram[df_ngram.index.duplicated()]) == 0\n",
        "eco_vocab, topic_vocab = get_vocab(df_ngram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RH-HEaA9L9y"
      },
      "outputs": [],
      "source": [
        "def get_model(df):\n",
        "    if \"vectorized\" not in df.columns.values:\n",
        "      df['vectorized'] = df.apply(count_vectorizer, axis=1)\n",
        "    df_train, df_test = train_test_split(df)\n",
        "    train_x = df_train['vectorized'].to_list()\n",
        "    test_x = df_test['vectorized'].to_list()\n",
        "    train_y = df_train['label']\n",
        "    test_y = df_test['label']\n",
        "\n",
        "    train_x = np.array(train_x)\n",
        "    test_x = np.array(test_x)\n",
        "    params = {'n_estimators': 386, 'max_depth': 13, 'min_samples_split': 9}\n",
        "    rf = RandomForestClassifier(**params)\n",
        "    rf.fit(train_x, train_y)\n",
        "    y_proba = rf.predict_proba(test_x)\n",
        "    y_pred = y_proba[:,1] > 0.50\n",
        "    print(precision_score(test_y, y_pred))\n",
        "    return rf\n",
        "\n",
        "def naive_filter(link, rf):\n",
        "    df = pd.read_csv(link)  # Changed index=\"id\" to index_col=\"id\"\n",
        "    df['clean_title'] = df['clean_title'].fillna(\"None None None\")\n",
        "    df['clean_text'] = df['clean_text'].fillna(\"None None None\")\n",
        "    df['vectorized'] = df.apply(count_vectorizer, axis=1)\n",
        "    x_data = np.array(df['vectorized'].tolist())\n",
        "    df['proba'] = rf.predict_proba(x_data)[:,1]\n",
        "    df['words'] = df['vectorized'].apply(get_word_list)\n",
        "    df['num_words'] = df['words'].apply(lambda x: len(x))\n",
        "    return df\n",
        "\n",
        "def get_trainset(corpus):\n",
        "  if corpus == \"rzepa\":\n",
        "    df = pd.read_csv(links_dataset[0], index_col=['id', 'translated'])\n",
        "    df.rename(columns = {\"class\" : \"label\"}, inplace = True)\n",
        "  else:\n",
        "    df_eco = pd.read_csv(materials_path / \"rzepa\" / \"eco_rzepa.csv\", index_col='id')\n",
        "    df_non_eco = pd.read_csv(materials_path / \"rzepa\" / \"non_eco_result.csv\", index_col='id')\n",
        "    df_eco['label'] = 1\n",
        "    df_non_eco['label'] = 0\n",
        "    df = pd.concat([df_eco, df_non_eco])\n",
        "    df['vectorized'] = df['vectorized'].apply(lambda x: x[1:-1].split(\" \"))\n",
        "    df['vectorized'] = df['vectorized'].apply(lambda data_list: [float(x) for x in data_list if x != ''])\n",
        "  df = df.fillna(\"None\")\n",
        "  return df\n",
        "\n",
        "def get_resulting_df(links, rf, corpus):\n",
        "    result = []\n",
        "    for link in tqdm(links):\n",
        "       result.append(naive_filter(link, rf))\n",
        "    df_result = pd.concat(result).set_index('id')\n",
        "    if len(df_result) < 5 * 10 ** 4  and corpus != \"dorzeczy\":\n",
        "      return df_result\n",
        "    else:\n",
        "      print(f\"{corpus}: {len(df_result)}\")\n",
        "      return df_result[df_result['proba'] > 0.1]\n",
        "\n",
        "def get_statistics(df_result):\n",
        "    for i, key in enumerate(stat_list[::-1]):\n",
        "      df_result[key] = df_result['vectorized'].apply(lambda x: x[-(i + 1)])\n",
        "    return df_result\n",
        "\n",
        "def save(corpus, df_result, df_eco, df_non_eco):\n",
        "    (materials_path / corpus).mkdir(parents=True, exist_ok=True)\n",
        "    df_eco.to_csv(materials_path/ corpus /\"eco_result.csv\")\n",
        "    df_non_eco.to_csv(materials_path/ corpus /\"non_eco_result.csv\")\n",
        "    df_result.to_csv(materials_path/ corpus /\"results.csv\")\n",
        "\n",
        "def determine_fate(df_result, corpus):\n",
        "  if corpus == 'rzepa':\n",
        "    df_eco = df_result[(df_result['proba'] > .9) & (df_result['ngram_sum_squared_to_total'] > 0.75) & (df_result['num_words']  > 3)]\n",
        "    df_non_eco = df_result[df_result['proba'] <= .25]\n",
        "  else:\n",
        "    df_eco = df_result[(df_result['proba'] > .5) & (df_result['ngram_sum_squared_to_total'] > 0.5) & (df_result['num_words']  > 2)]\n",
        "    df_non_eco = df_result[df_result['proba'] <= .2]\n",
        "  df_result = df_result[~ (df_result.index.isin(df_non_eco.index.values) | df_result.index.isin(df_eco.index.values)) ]\n",
        "  print((len(df_result), len(df_eco), len(df_non_eco)))\n",
        "  return df_result,df_eco,df_non_eco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFyAj2of9L9z",
        "outputId": "ccbf4d23-c642-4229-b5e4-17a102655de8"
      },
      "outputs": [],
      "source": [
        "def preprocess_corpus(corpus):\n",
        "    if (materials_path / corpus / \"eco_result.csv\").is_file() | (materials_path / corpus / \"non_eco_result.csv\").is_file() | (materials_path / corpus / \"results.csv\").is_file():\n",
        "      print(\"Done: \" + corpus)\n",
        "      return\n",
        "    else:\n",
        "      print(\"Processing: \" + corpus)\n",
        "    links = corpus_links[corpus]\n",
        "    df = get_trainset(corpus)\n",
        "    rf = get_model(df)\n",
        "    df_result = get_resulting_df(links, rf, corpus)\n",
        "    get_statistics(df_result)\n",
        "    df_result, df_eco, df_non_eco = determine_fate(df_result, corpus)\n",
        "    save(corpus, df_result, df_eco, df_non_eco)\n",
        "\n",
        "\n",
        "corpus_links = {\n",
        "    \"newsweek\" : links_newsweek,\n",
        "    \"rzepa\" : links_rzepa,\n",
        "    \"gpc\" : links_gpc,\n",
        "    \"wprost\" : links_wprost,\n",
        "    \"polityka\" : links_polityka,\n",
        "    \"dorzeczy\" : links_dorzeczy,\n",
        "    \"dataset\" : links_dataset,\n",
        "    \"wyborcza\" : links_gw,\n",
        "    \"wpolityce\" : links_wpolityce\n",
        "}\n",
        "for corpus in corpus_links.keys():\n",
        "  if corpus != 'dataset':\n",
        "    preprocess_corpus(corpus)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
