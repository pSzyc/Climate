{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "df = pd.read_csv(\"colocates.csv\", parse_dates= ['date'])\n",
    "nlp = spacy.load('pl_core_news_sm')\n",
    "df.colocate = df.colocate.apply(lambda colocate: \" \".join([token.lemma_ for token in nlp(colocate)]) )\n",
    "df['words'] = df.colocate.str.split(\" \")\n",
    "df_exploded = df.explode('words')\n",
    "df_exploded.drop('colocate', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = df_exploded.date.min()\n",
    "interval_days = 6 * 30\n",
    "end_date = start_date + datetime.timedelta(days=interval_days)\n",
    "max_date = df_exploded.date.max()\n",
    "corpus_list = []\n",
    "while start_date < max_date:\n",
    "    df = df_exploded[df_exploded.date.between(start_date, end_date)]\n",
    "    corpus_list.append(df)\n",
    "    end_date += datetime.timedelta(days=interval_days/2)\n",
    "    start_date += datetime.timedelta(days=interval_days/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows With Raw Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of collocates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words 15492\n"
     ]
    }
   ],
   "source": [
    "word_set = set()\n",
    "\n",
    "for corpus in corpus_list:\n",
    "    word_set.update(corpus.words)\n",
    "    \n",
    "print(f\"Number of unique words {len(word_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To establish the relative minimum collocation frequency, we first identify a subcorpus, which provides the smallest amount of evidence for collocation due to the lowest number\n",
    "of occurrences of the node in that particular subcorpus\n",
    "2. In other subcorpora with a higher frequency of the node, the minimum frequency threshold is proportionally stricter\n",
    "\n",
    "\n",
    "For example, if the threshold for the smallest subcorpus is set to be\n",
    "at least three co-occurrences of the collocate and the node, then the requirement\n",
    "for a subcorpus that includes twice the number of nodes would be to include at\n",
    "least twice this number of co-occurrences with the collocate (six in this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_counts = [corpus.words.value_counts() for corpus in corpus_list]\n",
    "node_counts = np.array([corp.text_id.unique().size for corp in corpus_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zmian\n",
      "i\n",
      "zmianach\n",
      "przed\n",
      "też\n",
      "globalne\n",
      "jednak\n",
      "które\n",
      "także\n",
      "coraz\n",
      "nie\n",
      "to\n",
      "ale\n",
      "są\n",
      "klimatycznych\n",
      "który\n",
      "zmianami\n",
      "dla\n",
      "a\n",
      "ze\n",
      "na\n",
      "tylko\n",
      "do\n",
      "co\n",
      "in\n",
      "zmiany\n",
      "już\n"
     ]
    }
   ],
   "source": [
    "def count_vector(word, corpus_counts = corpus_counts):\n",
    "    count_list = [corpus_count[word] if word in corpus_count else 0 for corpus_count in corpus_counts]    \n",
    "    return np.array(count_list)\n",
    "\n",
    "def count_as_colocate(c_vector, node_counts):\n",
    "    if (c_vector == 0).any():\n",
    "        return False\n",
    "    min_colocation_index = np.argmin(c_vector)\n",
    "    min_freq = np.min(c_vector)\n",
    "    count_min = node_counts[min_colocation_index].min()\n",
    "    for freq, node_count in zip(c_vector, node_counts):\n",
    "        if freq < min_freq * node_count / count_min:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "for word in word_set:\n",
    "    if count_as_colocate(count_vector(word), node_counts):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isPresentAtWindow(window, word):\n",
    "    return window[word] > 10\n",
    "\n",
    "vectors = []\n",
    "for window in df_by_window:\n",
    "    vector = [isPresentAtWindow(window, word) for word in results]\n",
    "    vector = np.array(vector)\n",
    "    vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.stack(vectors, axis = 0)\n",
    "time_data = pd.Series(time_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data = time_data.reset_index().rename(columns={0: 'date'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('vectors.npy',vectors)\n",
    "time_data.to_csv(\"time_periods.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"time_periods.csv\", index_col='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(\"vectors.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
